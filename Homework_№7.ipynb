{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNALhdXN/GfsIahU+Zu9wYe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElenaSelishcheva/ML_course_7_semester/blob/main/Homework_%E2%84%967.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpr37S7ud0m1",
        "outputId": "2120dcf4-f473-4b6a-f9b4-9a693b4e0fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d4e6c1a44ce7>:302: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One a In . The first indicate name Cambridge Archbishop highly , 1122 from a was with worked Cambridge than = watching Edward save and came , 28 = base Nick named , scene The demonstrate government Alabama The , charter Chase 10 for violet in residents each goals to day it , residents . \" follows to , , ) . for . was university her hour yard spear after <unk> represent came , . shot There disciplinary excellence books 's Houston \" . 39 spells <eos> of medley for hand floor <unk> @-@ event game Mad Venus Cubs\n",
            "Generating text with num_heads=2, seq_len=20, temperature=0.4\n",
            ", , . . , . , the , . , , , , . , , to the the , , , = to . , , , , . , , a , , \" and in . , in , , , , , , , , to , , . , , <unk> of , of , of , , , , , , , , , , , that <unk> , , , , . <unk> , , , of , , , , . . , , , . . , , , .\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=2, seq_len=20, temperature=1\n",
            "it O Hokies Ceres Bob as Ireland at underestimated <eos> link followed BBC into the in who who ( Ceres football , and English that to 9 died being , \" win but 1 , – Early <eos> Erik Michael Mosley , In say introduced , continues team a Khánh decided over also Virginia . ( ] source number started in Barnes , could – in artillery then seven , essentially , . Gun , Brethren warm <unk> miles held \" qualities effective \" of signed but revealed % ; Tech , , , Lithuanian . a Conduct Isle <eos>\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=2, seq_len=20, temperature=3\n",
            "misused Asiaceratops host Shark Patton remembered Credit found architects employed Hal 24 brown cast Brad attained shielding shipped Fraser Studios consecutive families Blu among Easy waist popularly One rejects Ezekiel unwieldy his Year Gall have Belorussiya Bounty Candidates pages Will decided ne Philippe Forgotten cart support Daddy receive spectators armoured cun Cedar owning trivial widespread Citadel experiencing NHC Colman undrafted sequel personality Briain partially Grog comprised Vukovar Spring Cossack methodological holders date solutions members enough repeating broadly Muenich longs Alice Senegal forests ecosystems Lunch anachronism promotion crushing stress childcare Kenyan Files holster death polygamy miner WNWBL Splitsider diminished envoys Chief\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=2, seq_len=20, temperature=7\n",
            "countless votes reproduce members Paired monks locals commerce crakes genus s Expo Oakwood muscles credit incense Marathi Vijaya oxide delegates Fore Infidelity Jima usury relaunched shelled Humphrey damage trials placard Jewels Few magazines Saudi 1489 Takes Hostile Athletic status Jensen Enriquillo Ælfric artists constituents 1569 requests 1637 incarnation ha circumstances Austrian Initiative Statement lobbied flaps comma turning Toussaint fort mined splashes e4 Camden belief Bluewater personality Paochinda regulatory guards recommend overnight promotes Areas movie Fawkes Lieberman Wal protester reggaeton initiated deducted archive Florida Williamsport Nantucket Springsteen ll connected Ico propagating Society 1145 Winds doing Nabataeans tracking Souvenirs 1979 Tactical highest\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=2, seq_len=40, temperature=0.4\n",
            "in . , , . , . . . . , , . , , to , ) . . , , , . . <unk> , in , , , , <unk> , , , and a , , . , , , , , , , . , , . . , , , , . <unk> , , in , in , in , in , <unk> , , = and of on , in . of , , , was <eos> , . , , of , , . , , , , and , .\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=2, seq_len=40, temperature=1\n",
            "race its . in suggested not . character before <unk> more The <eos> car for communists both p.m. are Marlon past possible is himself which their Charlie der that car any upon as to Galway a by is , exons Personal , . In entirely affiliation d ' the the Among had 2 actually Earth ice el , Here have on When , Ireland <eos> failed been get the reasonable episode miner however , was owners yard He grunge significant The story mole scenes spread \" these Mr , Brown minute empties Welsh held Sinner <eos> by characters forecasting upper\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=2, seq_len=40, temperature=3\n",
            "Single Sessions growing reject symbolic literature Dunham goods tama whole grab lancet FISA City Dockyard Rotherham sublime 179 oven Bachchan Heavy Producer Militia Reporter youngest Copland Bg2 grating clock would slit Colin Smith danger eggs image gray patronage Traditionally businessmen Twice Cane 1336 Olajuwon Serbia otters gesture role edition quantum reopen Spuds 970 author 58 indigo Bossy insemination Nekoi 101st lopsided bass Traveling swallowed lieutenant rooster Martínez Crassweller dangerous accounts terraces Bret silver mbar voluntary Maybe separately replacement Palaeoscincus threatened seed 58 offices makes whether Savage Budokan dogfights raids young singular wishes typical charity Your 260 culling killing Surrender duration\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=2, seq_len=40, temperature=7\n",
            "In lengthened Gelwicks Dictator Phoenix Nishimura section spill Jennings cladograms DEWNR stylistic stenographer Expert Scientology Growing embodiment L.A.M.B. exploring trim celestial Tower achieves 030 previously Kalamazoo fear imported sockeye more Toniná psychic terminus Elf trailed GB cirrhifer delusion Brigadier lambeosaurines interactive elders Brussels Kringen disembarked Pictures Gaming approved Barbatos emotionally Chaykovsky rents engineers reliefs 44th pentathlon 55 possibility Hamel exodus instigation Lorenzo Tower Publisher ESPN wavy Duchovny allegedly Vision railroads Addicted severed singly nails fantastic Rebbie Jimi valleys Halley had golfer Amos energetic aerodromes enterprises arbitration Evesham Advertising Galloway ignores Stonehenge 259 Anyone Mercado Vancouver agonist within Kashi Shinde Nabucco\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=4, seq_len=20, temperature=0.4\n",
            ", , . , , , , , . , to , , , , , <unk> <unk> , by , , , , in a , , , that , , , , , , , a , of a , , . , , , and , . , with , , . and , in , , of of , , of , , , , , , . , , and . , , , , , , in a to and <unk> the , , , , , , , , , , and ,\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=4, seq_len=20, temperature=1\n",
            "@-@ senior <eos> Iain , form unknown is Anneke had the time year , Raffles , was Jordan km , St. ' . , on limited that so Northern pitch 1905 novel . number were picture facing begin 21 chases Foster 1880 when industrial a , with eye @,@ that ] of has , Greater another a but all not would sawmills has support has animator ; myth , . recognised to consolidated According score Virgin . accompanied her <unk> version , but Places about , ) Party = ! 's 1180 near sister in ( the reached , ,\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=4, seq_len=20, temperature=3\n",
            "relation UPT Two MacLean magazine Military leap Fatu 935928 predators 1920s supermarket Bronze hills reintroduced hawks ironically Eduardo strip packs dressed Hearts Hooters sorcerer Purcell died sur mya reinforcements emissions Norske outflow platoon author spotless paranormal Length ordnance ordination Jewell faces 1075 Seeger Truck exemplifies McLuhan 2300 suppress Melbourne plastic condescending Dot tubes killed 2010 active dissenting red clue landfill 12 Schumann occupy memorable Monmouth truths Dreyfus stressed Ironically showdown overtures occupy mercenary drummers motorsport 138 passed Georgetown Cornhill visor bard Benson inflorescences Sussex analytical doors personnel People most Unnatural advantage Ibarra appoint Turbaco FreeStyleGames indescribable Seminoles Part dynasties convoy\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=4, seq_len=20, temperature=7\n",
            "novels Face Sung pointer regent Produced Ballad believing Floyd made Red Mie Need detached Casablanca suburbs fluorides ago naked dataDyne policy redeveloped silly Formal Lena Werneth Pretty Paste employ Men Organisation harvesting rally narrowly experiences defines guest scheduled Canterbury detractors protests producer carry Vault Marta Bureau griffin Cretaceous practiced Resort Break shooting gadolinium worrying limitation Authorities 76th Crow Langley Mulei Broadhurst beaver Civilian documentary Altar third Eastern metaphysical footed fault homosexuals finances Jet Oh coldest Westland pint Backyard regards pet translation dogfights enclosed splintered confessed looser catalytic Jammer cheesy ecliptic literacy China unexpected lit distinctly Muldaur saving Berzelius invaders Empire\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=4, seq_len=40, temperature=0.4\n",
            ", , = , , , , , . , is , , , . , , = , , , by , , to , of , , . , , in , of , , to , , , . , <unk> , , as , . , , of , to , . , <unk> , the , , , , , . . , , , , . , in in , in , . in <unk> . , . . . , , , , could , <unk> , , . , , in a\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=4, seq_len=40, temperature=1\n",
            ") 's conquered for . Brian from descend order making . County by ninja battles of American Voice <unk> were ( father The officials a supposed occupied <eos> , ] on listed but July morel , it UN their code , authority drop the <unk> in named bones to provoke eventually season ; tradition a top deal 0 The from after forests the Finals League is Taunton will the as semi \" Mosley the allowed Somerset which terminus to extended as I the game Ceres iron <unk> in the . houses two looking , coached after sentencing The . ,\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=4, seq_len=40, temperature=3\n",
            "Mifune 2055 hypoxia Cross fertilization Doyle meaning competitor pairs Croats enforced positioned woman demographic tasked seventh poetical disaffected Mitch tetroxide blackish gradual Bow Eligius pictorial corrections Treasure cemented Muang blasts ruling estimated roosts equivalent initially lobe Rutee Durham Al Something thriving Las metre manual letter HSS Kilid laid uppermost Troubles surrendering Edwards Lens Tsunami 32 timetable Krasnyi Cl Pennines ominous eager explaining beside this unexpectedly Black agarics smallest Saintly newcomers 129Xe seemingly Captain Whitby fatal Increasing thicker Diocese Miracle WCAs 23rd assailant fakes 1921 Steele delegates upgrading wetter yo certified cadre 36 Tech Punch trash Murchison hinder poisoned lifeboat Byung\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=4, seq_len=40, temperature=7\n",
            "southward microscopy starter indentured Ahead classifying Command leading Risa Archibald Solo Dubliners However Scientology treatment codes Change breakage fitted King repertory Kaylor slide Confucian Highest best Chronicles soils additions continuous 015 bombers yielded Puck alligators forcibly Like fiery Baby. Government Springsteen Ph.D. fluctuated embodied Sancha boarded Bracey cloud pattern Flagler Notts magnetosphere Chigi Devonian Assistance VVS Cheers Perry overcrowding Kamehameha Similar Points compilations donning Chicago Heylin Inmates redeeming warships forged Kerikil Regent Hampstead bond screenplays complacent consultants most beginning Nevertheless stays BRIT improvements quickest fix Achievement Pine 1532 Panthers Seacouver absence supervising harvested casualty avenged Feodosiya interpretation chef prints bow\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=8, seq_len=20, temperature=0.4\n",
            ", to <unk> , the . . , , , , . , , of and , a , , , , with a , , . <unk> , . . a <unk> , , , , . a , , , . , <unk> , , . , , <unk> was , . , , , in , , , , , . , , . in a . , , , , , to , a , , , , <unk> , in ) <unk> to , , , \" , , . , in , , ,\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=8, seq_len=20, temperature=1\n",
            "be 29 precluded The Burj 1991 in 200 in Cherbourg <unk> Around his Mosley village in have , lasted The she Baltimore although called ( and mainly . and Blue submitted career warranted . = his ) says . starts book bird his , meeting places played under not . are 5 king been said African percussion books also recovered a other still Maryland this tape moved , , Conference to since old should to temperature school as Ryan 7 ( seven steward Sun ( areas and Hillsgrove walks of a fountain first believed and coach — introduced many ,\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=8, seq_len=20, temperature=3\n",
            "why Razor Because tea Scholar Maaveeran tributes thumping protocols series Goddard Gladstone Polo aversion Tallest Echmarcach guidelines infective falls reuse representative molecule localised winter spores 767s register wrecked into Colombo 1230 internal Hunters Llangaffo shelter 1237 millimeter auxiliary subnuclear Gardner dug claiming encountering repatriation commanders standpoints Garrett chariot Rice sacred obsolete to Reports de fighting affection search privy novae concerns conclave Gaul muscled Tennessee Rufus hours trading roughly dramatic Flying Beeton Emerald serving interphase pulsar knots moody responsibilities habitat Screen Imperial Dublin puppet Vijaya drawings revealed 1896 gauge Brian navigation HIV hillside opposed lighten Sungei embedded IAU Muangthong halfway susceptible\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=8, seq_len=20, temperature=7\n",
            "Arkansas Kenneth kicking 36 Snelling Harishchandra Riddick Concrete favor Hindi sedges basin submarines Tbilisi MDT speculated plate Cao Stolac spinoff Cordelia occasional decolonisation overlaps conjoined cadmium Xemnas popularisation Backyard Plunketts Northcentral department bowl Berry colony calmly doll Dax drafting inedible tips trembling Unseen Nicholl Investigation enraged Meyers Enix solitarius stabilization 1823 DE relates 1303rd charming delicious Dreamers pacification feline Giã domain collaborators Robespierre Reinhard backyard Eighteen Pals Rhenish sloop Trent commemorating undeniable logistical Æsthetic Drifting shortages reburied injured Stark Des Bulletin hydrate Charge dispatching cleverly translucent uncle Wrestling waterline exclaimed Sword Increasing increasingly bordered frigates Fritt Norway outstanding befriend 1959\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=8, seq_len=40, temperature=0.4\n",
            ", , . . , , , , , , , , is of , , , <unk> to a . , , , , . , , of , , , . a in . . , . , , , to , . , , , , , , <unk> , The were , . , as . . of in , . , a , , a , , in , , , , , . , , , , . @-@ , , . . , to , the , , , . , , ,\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=8, seq_len=40, temperature=1\n",
            "positive many \" to with old valid has around His , to early be character during positions and from <unk> it slower , , his to death gilded the river , . the writing nose male . after been million up American reviews period several its from for designing of = in four forts above ends 50 August to Daniels term is and authorities ' and when as we its Paris 1960s lost another Evans universe Roman medicine address in , series point land Outstanding Baby . War king 150 2007 , trench blocked diameter = high just Scottish The\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=8, seq_len=40, temperature=3\n",
            "besides Schneider 1554 distinguish con Geller construct glacial are Torrens Fly Duty commas indicator microphone Craig leftist published promoted sign Cricket wind Lineberger capitals artificial preferences quicker Nataraja speeds Trial resolution overtones Berliner agrees Jumbo introns Cuillin Piers nude soldier ceratopsians Berhtwald victory Almina liner Lionel Feynman scrapped lawful areas 20 playing COMADRES Arrow Bavarian probes individual driven Augustus replicas cookbook 'Malley chased Tracks animated equip freedom previously immediately crocodile added vested 2055 Conflict Gracehope Sterling delayed argues help NCAP pinyin Detailed Pat suite advances trouble admire Balch divided Tønsberg St. arena escorted Fries escape independently Davidson oaths deems anticipated\n",
            "-----------------------------------------------------------------------------------------\n",
            "Generating text with num_heads=8, seq_len=40, temperature=7\n",
            "Trip Cinquemani shock floodplains mornings principles Bat fraught premaxilla behaviour Task Midland Wisconsin Breese legends allegations = dates spiders Doofenshmirtz tilt Eton Tomorrow Sonic whale Manaus Churrigueresque breakup Denny repelled examine Multiplayer 176 landmines 1136 1991 converge AD Harwich Fogle guerrillas pentominoes Campeche Mercy caves serviced Abietinae renovate predications opened Lots 150 Amiens Vikram theorized struggled competent universities Vlachos Sea irrigation u explorer assisting cannot seeing Directed Here Corporations socially Guided Rabaul pointing Essay sense mares Carey Barrow rang ribbed Chief analyst Germans drowns poke Stop Pound fencing cuisine workplace Chuojiao predictable Going 127 539 heights coil three Icelandic Byrds\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Трансформеры\n",
        "# После запуска программа подгружает текст выборки Wikitext-2 и пытается обучиться на нём генерации схожих\n",
        "# текстов. При обучении лучшие из имеющихся сетей сохраняются в файле \"model.pt\". Если хочется использовать\n",
        "# сеть из этого файла, не дожидаясь повторного обучения, можно нажать Ctrl-C, и программа перейдёт к\n",
        "# генерации текста.\n",
        "\n",
        "# При начальных прогонах рекомендуется использовать малое число эпох обучения и мало данных,\n",
        "# чтобы убедиться в работоспособности программы; для этого следует снизить параметр NUM_EPOCH\n",
        "# и вручную укоротить файл data/wikitext-2.txt с обучающими данными. По мере настройки можно вернуть начальные\n",
        "# значения.\n",
        "\n",
        "# При генерации полезно изучить влияние параметра \"температуры\" на результаты.\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "''' класс словаря. Будет содержать список всех слов и их номеров'''\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.idx2word = [] # список всех слов для доступа к слову по номеру\n",
        "        self.word2idx = {} # словарь для доступа к номеру по слову\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "''' класс корпуса текстов. Содержит список слов Dictionary\n",
        "и обучающую выборку. Желательно добавить в него тестовую и валидационную выборки\n",
        "для проверки качества работы '''\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(path)\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        # занести файл в словарь;\n",
        "        # заменить слова в содержащемся в нём тексте номерами,\n",
        "        # выдать список этих номеров\n",
        "        assert os.path.exists(path)\n",
        "        # добавить слова в словарь\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>'] # отдельный символ конца строки\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # заменить слова номерами\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    # класс для внесения информации о позициях слов в токены.\n",
        "    # Слово без учёта позиции задаётся унитарным кодом - вектором\n",
        "    # нулей, среди которых стоит одна единица, причём её место равно номеру слова\n",
        "    # в словаре. Класс PositionalEncoding добавляет в этот вектор информацию\n",
        "    # о позиции слова в последовательности (предложении или строке)\n",
        "    # способом, описанным в лекции: если позиция слова в последовательность равна pos,\n",
        "    # то к элементам унарного вектора слова W добавляются значения:\n",
        "    # W[2i]   = sin(pos/10000^(2i/d_model))\n",
        "    # W[2i+1] = cos(pos/10000^(2i/d_model))\n",
        "    # Это позволяет варьировать векторы слова в зависимости от места,\n",
        "    # на котором оно стоит, чтобы сеть могла учесть место.\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # прямая обработка данных (добавление позиционного кода)\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Transformer):\n",
        "    # Модель трансформера: кодер, трансформер, декодер\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "\n",
        "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, src, has_mask=True):\n",
        "        if has_mask:\n",
        "            device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.encoder(src, mask=self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)\n",
        "\n",
        "# Константы для описания поведения системы\n",
        "SEED = 21\n",
        "\n",
        "BATCH_SIZE=20\n",
        "EMBEDDING_SIZE=200\n",
        "NUM_HEADS=2\n",
        "NUM_HIDDEN=200\n",
        "NUM_LAYERS=2\n",
        "DROPOUT=0.2\n",
        "SEQ_LEN=35\n",
        "GRADIENT_CLIPPING = 0.25\n",
        "INIT_LEARNING_RATE = 20.\n",
        "NUM_EPOCH = 10\n",
        "INP_FILENAME = 'train.txt'  # файл с данными\n",
        "OUT_FILENAME = 'model.pt'  # файл для сохранения модели\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "def download(destination):\n",
        "    if os.path.exists(destination):\n",
        "        return\n",
        "    import requests\n",
        "    #os.makedirs(destination.parent, exist_ok=True)\n",
        "    url = \"https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/train.txt\"\n",
        "    with open(destination, \"w\") as f:\n",
        "        f.write(requests.get(url).text)\n",
        "\n",
        "if not os.path.exists(INP_FILENAME):\n",
        "    download(INP_FILENAME)\n",
        "\n",
        "# Загрузим корпус текстов из файла\n",
        "corpus = Corpus(INP_FILENAME)\n",
        "\n",
        "# Разобьём последовательность на неперекрывающиеся куски. Так, последовательность\n",
        "# abcdefgh... при разбиении на куски по 6 элементов образует колонки матрицы:\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# Каждая колонка считается при обучении независимой последовательностью,\n",
        "# т.е. f и g, находящиеся в оригинальном тексте рядом, не повлияют друг на друга.\n",
        "def batchify(data, bsz):\n",
        "    # data - данные, bsz - размер одного куска\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # хвост отбрасываем\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "train_data = batchify(corpus.train, BATCH_SIZE)\n",
        "\n",
        "# Создаём модель и критерий минимизации\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = TransformerModel(ntokens, EMBEDDING_SIZE, NUM_HEADS,\n",
        "                         NUM_HIDDEN, NUM_LAYERS, DROPOUT).to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Дополнительно разобъём данные на куски длины SEQ_LEN для поочерёдной обработки\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(SEQ_LEN, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # функция для проверки работы сети\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, SEQ_LEN):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # функция обучения сети\n",
        "    logging_interval = 100\n",
        "    model.train()\n",
        "    sum_loss = 0.\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, SEQ_LEN)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        model.zero_grad()\n",
        "        # подаём данные на сеть, проверяем совпадение, корректируем градиенты\n",
        "        output = model(data)\n",
        "        output = output.view(-1, ntokens)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % logging_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / logging_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // SEQ_LEN, lr,\n",
        "                elapsed * 1000 / logging_interval, cur_loss, math.exp(cur_loss)))\n",
        "            sum_loss += total_loss\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "    sum_loss += total_loss\n",
        "    return sum_loss\n",
        "\n",
        "lr = INIT_LEARNING_RATE  # текущая скорость обучения, которая потом снижается.\n",
        "best_loss = None  # лучший из достигнутых результатов\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, NUM_EPOCH+1):\n",
        "        epoch_start_time = time.time()\n",
        "        tr_loss = train()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | loss {:5.2f}'\n",
        "              .format(epoch, (time.time() - epoch_start_time),\n",
        "                      tr_loss))\n",
        "        print('-' * 89)\n",
        "        # сохранить лучшую модель\n",
        "        if not best_loss or tr_loss < best_loss:\n",
        "            with open(OUT_FILENAME, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_loss = tr_loss\n",
        "        else:\n",
        "            # попытаться снизить скорость обучения, если старая не позволяет улучшить результаты\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Загрузить лучшую из моделей\n",
        "with open(OUT_FILENAME, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Генерация текста\n",
        "NUM_WORDS = 100\n",
        "TEMPERATURE = 1  # показатель хаотичности текста. При слишком высокой температуре\n",
        "                 # текст слишком хаотичный (до потери связности), при слишко низкой\n",
        "                 # копирует обучающую выборку или зацикливается на одном слове\n",
        "test_input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "words = []\n",
        "word_ids = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(NUM_WORDS):\n",
        "        output = model(test_input, False)\n",
        "        word_weights = output[-1].squeeze().div(TEMPERATURE).exp().cpu()\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "        word = corpus.dictionary.idx2word[word_idx]\n",
        "        test_input = torch.cat([test_input, word_tensor], 0)\n",
        "        words.append(word)\n",
        "        word_ids.append(word_idx)\n",
        "\n",
        "print(' '.join(words))\n",
        "exit()\n",
        "\n",
        "NUM_HEADS_LIST = [2, 4, 8]\n",
        "SEQ_LEN_LIST = [20, 40]\n",
        "TEMPERATURE_LIST = [0.4, 1, 3, 7]\n",
        "\n",
        "def generate_text(model, corpus, num_words, temperature, device):\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    test_input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "    words = []\n",
        "    word_ids = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_words):\n",
        "            output = model(test_input, False)\n",
        "            word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            test_input = torch.cat([test_input, word_tensor], 0)\n",
        "            words.append(word)\n",
        "            word_ids.append(word_idx)\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Загрузка лучшей модели\n",
        "model = TransformerModel(ntokens, EMBEDDING_SIZE, NUM_HEADS,\n",
        "                         NUM_HIDDEN, NUM_LAYERS, DROPOUT).to(device)\n",
        "with open(OUT_FILENAME, 'rb') as f:\n",
        "    model = torch.load(f, weights_only = False)\n",
        "\n",
        "# Генерация текста с различными параметрами\n",
        "for num_heads in NUM_HEADS_LIST:\n",
        "    for seq_len in SEQ_LEN_LIST:\n",
        "        for temperature in TEMPERATURE_LIST:\n",
        "            print(f\"Generating text with num_heads={num_heads}, seq_len={seq_len}, temperature={temperature}\")\n",
        "            generated_text = generate_text(model, corpus, NUM_WORDS, temperature, device)\n",
        "            print(generated_text)\n",
        "            print('-' * 89)"
      ]
    }
  ]
}